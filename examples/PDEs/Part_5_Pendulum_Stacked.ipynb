{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zPjMocS6JIUz"
   },
   "source": [
    "# Stacked Physics-Informed Neural Networks (Stacked PINNs) in Neuromancer\n",
    "\n",
    "This tutorial demonstrates the use of stacked (multi-fidelity) PINNs for solving challenging multiscale differential equations in the Neuromancer library.\n",
    "\n",
    "<img src=\"../figs/multifidelity_pinn.png\" width=\"1200\">  \n",
    "\n",
    "Despite the success of physics-informed neural networks, the \"vanilla\" formulation of PINNs [3] tend to fail to learn the solution of certain dynamical systems. In particular, in the presence of multiple fixed points and/or highly oscillatory, multiscale systems, the learned solution might converge to the trivial solution.\n",
    "\n",
    "In this notebook, we demonstrate how to solve these classes of problems using Stacked PINNs. This method consists of stacking multifidelity networks [1,2] in a composition manner: starting from a single-fidelity network (Step 0, c.f. figure above), the output is fed into a multifidelity block, consisting of two networks: a linear network (with no activation functions), and a nonlinear network. The nonlinear network takes as input the outputs of Step 0 plus the original inputs. The result of the multifidelity layer is a convex combination of the outputs of linear and nonlinear networks, weighted by a learnable parameter $\\alpha$ as shown below:\n",
    "\n",
    "<img src=\"../figs/multifidelity_detail.png\" width=\"600\">  \n",
    "\n",
    "In this architecture, the output of each of the stacking layers can be seen as a lower fidelity model for the next layer. This composition process progressively improves the learned solution.\n",
    "\n",
    "In the example below, we demonstrate the example of a damped pendulum, where the classical formulation of PINNs fail to converge to the correct solution, and we show how the stacked model allows us to solve the problem with higher accuracy.\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "[1] [Howard, Amanda A., et al. (2023) Stacked networks improve physics-informed training: applications to neural networks and deep operator networks.](https://arxiv.org/abs/2311.06483)\n",
    "\n",
    "[2] [Heinlein, Alexander, et al. (2023) Multifidelity domain decomposition-based physics-informed neural networks for time-dependent problems.](https://arxiv.org/abs/2401.07888)\n",
    "\n",
    "[3] [Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2017). Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations.](https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y61YA90-WIZ1"
   },
   "source": [
    "## Install (Colab only)\n",
    "Skip this step when running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WZrPCr9GWEAJ",
    "outputId": "d0ff6dfe-de2a-4675-a36c-e2a7fce486d9"
   },
   "outputs": [],
   "source": [
    "# !pip install \"neuromancer[examples] @ git+https://github.com/pnnl/neuromancer.git@master\"\n",
    "# !pip install pyDOE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.modules import blocks\n",
    "from neuromancer.system import Node\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.trainer import Trainer\n",
    "from neuromancer.dynamics.integrators import integrators\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "**The Pendulum Problem** is a fundamental example in classical mechanics, where the equations of motion describe the behavior of a simple pendulum under the influence of gravity and damping. The problem is given by the system of first-order ordinary differential equations:\n",
    "\n",
    "$$\n",
    "\\frac{ds_1}{dt} = s_2,\n",
    "$$\n",
    "$$\n",
    "\\frac{ds_2}{dt} = -\\frac{b}{m}s_2 - \\frac{g}{L}\\sin(s_1).\n",
    "$$\n",
    "\n",
    "Here, $s_1$ represents the angular displacement, and $s_2$ represents the angular velocity of the pendulum.\n",
    "\n",
    "**Initial Conditions:**\n",
    "\n",
    "The initial conditions are given by:\n",
    "$$ s_1(0) = 0.5, $$\n",
    "$$ s_2(0) = 0.5. $$\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "The parameters for this problem are:\n",
    "- Mass of the pendulum $ m = 1 $ kg,\n",
    "- Length of the pendulum $ L = 1 $ m,\n",
    "- Damping coefficient $ b = 0.05 $ kg/s,\n",
    "- Acceleration due to gravity $ g = 9.81 $ m/sÂ²,\n",
    "- Total simulation time $ T = 20 $ s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "m = 1.0    # Mass (kg)\n",
    "L = 1.0    # Length (m)\n",
    "b = 0.05   # Damping coefficient (kg/s)\n",
    "g = 9.81   # Acceleration due to gravity (m/s^2)\n",
    "T = 20.0   # Total simulation time (s)\n",
    "\n",
    "# Initial conditions\n",
    "s1_0 = 0.5\n",
    "s2_0 = 0.5\n",
    "\n",
    "# Number of collocation points\n",
    "N_cp = 500\n",
    "\n",
    "# Time discretization\n",
    "t = torch.linspace(0, T, N_cp).to(device)\n",
    "\n",
    "# Initial conditions\n",
    "ic_t = t[0].unsqueeze(0).unsqueeze(0)\n",
    "ic_s1 = torch.tensor([s1_0]).unsqueeze(0).to(device)\n",
    "ic_s2 = torch.tensor([s2_0]).unsqueeze(0).to(device)\n",
    "\n",
    "# Collocation points\n",
    "t_train_cp = t[1:].unsqueeze(1)\n",
    "\n",
    "# Combine initial condition and collocation points for training\n",
    "t_train = torch.cat((ic_t, t_train_cp)).float().to(device)\n",
    "s_train = torch.cat((torch.tensor([[s1_0, s2_0]]).to(device), torch.zeros((t_train_cp.shape[0], 2)).to(device)))\n",
    "\n",
    "t_train.requires_grad_(True)\n",
    "\n",
    "# Create dataset\n",
    "train_data = DictDataset({'t': t_train}, name='train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture\n",
    "\n",
    "In this part, we build two networks: a vanilla PINN, and the stacked PINN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net to solve the ODE problem\n",
    "# Variables in: t; variables out: s (s1, s2)\n",
    "net_vanilla = blocks.MLP(insize=1, outsize=2, hsizes=[200]*3, nonlin=nn.SiLU).to(device)\n",
    "\n",
    "net_stacked = blocks.StackedMLP(insize=1, outsize=2, n_stacked_mf_layers= 4, h_sf_size=[100]*3, nonlin=nn.SiLU,\n",
    "                                 h_nonlinear_sizes=[50]*5, h_linear_sizes=[20,20],\n",
    "                                 #freeze_epochs=[10000, 20000, 30000, 40000], \n",
    "                                 verbose=True).to(device)\n",
    "\n",
    "# Symbolic wrapper of the neural net\n",
    "# Inputs: t\n",
    "# outputs: s (s1, s2)\n",
    "ode_net_vanilla = Node(net_vanilla, ['t'], ['s'], name='net_vanilla')\n",
    "ode_net_stacked = Node(net_stacked, ['t'], ['s'], name='net_stacked')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Symbolic Neuromancer variables\n",
    "s = variable('s')\n",
    "s1 = s[:, [0]]  # Angular displacement\n",
    "s2 = s[:, [1]]  # Angular velocity\n",
    "t = variable('t')  # Time\n",
    "\n",
    "# Define the ODE system\n",
    "ds1_dt = s2\n",
    "ds2_dt = -(b/m) * s2 - (g/L) * torch.sin(s1)\n",
    "\n",
    "# Define the PINN form\n",
    "f_pinn1 = s1.grad(t) - ds1_dt\n",
    "f_pinn2 = s2.grad(t) - ds2_dt\n",
    "f_pinn1_grad = f_pinn1.grad(t)\n",
    "f_pinn2_grad = f_pinn2.grad(t)\n",
    "\n",
    "# Scaling factors for better convergence\n",
    "scaling_ic_vanilla = 1.0\n",
    "scaling_cp_vanilla = 1.0\n",
    "scaling_ic_stacked = 1.0\n",
    "scaling_cp_stacked = 10.0\n",
    "\n",
    "# ODE collocation points loss (MSE)\n",
    "l_cp1_vanilla = scaling_cp_vanilla * (f_pinn1 == torch.tensor(0.).to(device))^2\n",
    "l_cp2_vanilla = scaling_cp_vanilla * (f_pinn2 == torch.tensor(0.).to(device))^2\n",
    "\n",
    "l_cp1_stacked = scaling_cp_stacked * (f_pinn1 == torch.tensor(0.).to(device))^2\n",
    "l_cp2_stacked = scaling_cp_stacked * (f_pinn2 == torch.tensor(0.).to(device))^2\n",
    "\n",
    "# Initial condition loss (MSE)\n",
    "l_ic1_vanilla = scaling_ic_vanilla * (s1[0] - ic_s1 == torch.tensor(0.).to(device))^2\n",
    "l_ic2_vanilla = scaling_ic_vanilla * (s2[0] - ic_s2 == torch.tensor(0.).to(device))^2\n",
    "l_ic1_stacked = scaling_ic_stacked * (s1[0] - ic_s1 == torch.tensor(0.).to(device))^2\n",
    "l_ic2_stacked = scaling_ic_stacked * (s2[0] - ic_s2 == torch.tensor(0.).to(device))^2\n",
    "\n",
    "# Combined loss\n",
    "loss_vanilla = PenaltyLoss(objectives=[l_cp1_vanilla, l_cp2_vanilla, l_ic1_vanilla, l_ic2_vanilla], constraints=[])\n",
    "loss_stacked = PenaltyLoss(objectives=[l_cp1_stacked, l_cp2_stacked, l_ic1_stacked, l_ic2_stacked], constraints=[])\n",
    "\n",
    "# Construct the PINN optimization problem\n",
    "problem_vanilla = Problem(nodes=[ode_net_vanilla], loss=loss_vanilla, grad_inference=True)\n",
    "problem_stacked = Problem(nodes=[ode_net_stacked], loss=loss_stacked, grad_inference=True)\n",
    "\n",
    "# Optimizer\n",
    "initial_lr = 1e-3\n",
    "optimizer_vanilla = torch.optim.AdamW(problem_vanilla.parameters(), lr=initial_lr)\n",
    "optimizer_stacked = torch.optim.AdamW(problem_stacked.parameters(), lr=initial_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print parameter counts\n",
    "print(f\" Total number of parameters in single-fidelity net= {sum(p.numel() for p in net_vanilla.parameters())}\")\n",
    "print(f\" Total number of parameters in multi-fidelity net= {sum(p.numel() for p in net_stacked.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print vanilla net\n",
    "print(net_vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stacked net\n",
    "print(net_stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train \"vanilla\" net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "epochs_vanilla = 50001\n",
    "\n",
    "# Neuromancer trainer\n",
    "trainer_vanilla = Trainer(\n",
    "    problem_vanilla.to(device),\n",
    "    torch.utils.data.DataLoader(train_data, batch_size=t_train.shape[0],\n",
    "                                collate_fn=train_data.collate_fn, shuffle=False),\n",
    "    optimizer=optimizer_vanilla,\n",
    "    epochs=epochs_vanilla,\n",
    "    epoch_verbose=5000,\n",
    "    train_metric='train_loss',\n",
    "    dev_metric='train_loss',\n",
    "    eval_metric=\"train_loss\",\n",
    "    warmup=epochs_vanilla,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train PINN\n",
    "best_model_vanilla = trainer_vanilla.train()\n",
    "\n",
    "# Load best trained model\n",
    "problem_vanilla.load_state_dict(best_model_vanilla)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train stacked net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "epochs_stacked = 50001\n",
    "\n",
    "# Neuromancer trainer \n",
    "trainer_stacked = Trainer(\n",
    "    problem_stacked.to(device),\n",
    "    torch.utils.data.DataLoader(train_data, batch_size=t_train.shape[0],\n",
    "                                collate_fn=train_data.collate_fn, shuffle=False),\n",
    "    optimizer=optimizer_stacked,\n",
    "    epochs=epochs_stacked,\n",
    "    epoch_verbose=5000,\n",
    "    train_metric='train_loss',\n",
    "    dev_metric='train_loss',\n",
    "    eval_metric=\"train_loss\",\n",
    "    warmup=epochs_stacked,\n",
    "    device=device,\n",
    "    multi_fidelity=True\n",
    ")\n",
    "\n",
    "# Train PINN\n",
    "best_model_stacked = trainer_stacked.train()\n",
    "\n",
    "# Load best trained model\n",
    "problem_stacked.load_state_dict(best_model_stacked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "\n",
    "# Single-fidelity predictions\n",
    "s_pred_vanilla = ode_net_vanilla(train_data.datadict)['s'].detach().cpu().numpy()\n",
    "s1_pred_vanilla = s_pred_vanilla[:, 0]\n",
    "s2_pred_vanilla = s_pred_vanilla[:, 1]\n",
    "t_train = train_data.datadict['t'].detach().cpu().numpy()\n",
    "\n",
    "# Multi-fidelity predictions\n",
    "s_pred_stacked = ode_net_stacked(train_data.datadict)['s'].detach().cpu().numpy()\n",
    "s1_pred_stacked = s_pred_stacked[:, 0]\n",
    "s2_pred_stacked = s_pred_stacked[:, 1]\n",
    "\n",
    "# Solve the ODE numerically in Neuromancer using 4th order Runge-Kutta\n",
    "class PendulumODE(nn.Module):\n",
    "    def __init__(self, b, m, g, L, insize=2, outsize=2):\n",
    "        super().__init__()\n",
    "        self.b = b\n",
    "        self.m = m\n",
    "        self.g = g\n",
    "        self.L = L\n",
    "        self.in_features = insize\n",
    "        self.out_features = outsize\n",
    "\n",
    "    def forward(self, x, t=None):\n",
    "        s1 = x[:, [0]]\n",
    "        s2 = x[:, [1]]\n",
    "        ds1_dt = s2\n",
    "        ds2_dt = -self.b / self.m * s2 - self.g / self.L * torch.sin(s1)\n",
    "        return torch.cat([ds1_dt, ds2_dt], dim=-1)\n",
    "\n",
    "pendulum_ode = PendulumODE(b, m, g, L)\n",
    "fxRK4 = integrators['RK4'](pendulum_ode, h=T/N_cp)\n",
    "y0 = torch.tensor([[s1_0, s2_0]])\n",
    "t_eval = torch.linspace(0, T, N_cp)\n",
    "\n",
    "# Solve the ODE numerically using RK4 integrator\n",
    "y_t = y0\n",
    "sol = [y_t]\n",
    "for t in t_eval[1:]:\n",
    "    y_t = fxRK4(y_t)\n",
    "    sol.append(y_t)\n",
    "sol = torch.stack(sol).detach().numpy()\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Angular Displacement\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t_train, s1_pred_vanilla, label='Predicted $s_1(t)$ (PINN)')\n",
    "plt.plot(t_train, s1_pred_stacked, label='Predicted $s_1(t)$ (Stacked PINN)')\n",
    "plt.plot(t_eval, sol[:,0,0], label='Numerical $s_1(t)$', linestyle='dashed')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Angular Displacement [rad]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Angular Velocity\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(t_train, s2_pred_vanilla, label='Predicted $s_2(t)$ (PINN)')\n",
    "plt.plot(t_train, s2_pred_stacked, label='Predicted $s_2(t)$ (Stacked PINN)')\n",
    "plt.plot(t_eval, sol[:,0,1], label='Numerical $s_2(t)$', linestyle='dashed')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Angular Velocity [rad/s]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that final parameter alpha is small (see Eq. 11 of Howard, Amanda A. et al. 2023).\n",
    "for idx,alpha in enumerate(net_stacked.alpha):\n",
    "    print(f\"alpha_{idx} = {alpha}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y61YA90-WIZ1",
    "UoqCzgLSp61M",
    "i0j73GoH86-m",
    "pOe9yRvxjakj"
   ],
   "machine_shape": "hm",
   "name": "4.DiffusionEquation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neuromancer10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
